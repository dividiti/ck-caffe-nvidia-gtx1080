{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [cknowledge.org/ai](https://cknowledge.org/ai): Crowdsourcing benchmarking and optimisation of AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PUBLIC] Benchmarking Caffe on NVIDIA GTX 1080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** The benchmarking results are released with approval from General Motors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Overview](#overview)\n",
    "1. [See the code](#code) [for developers]\n",
    "1. [Get the data](#data) [for developers]\n",
    "1. [See the tables](#tables)\n",
    "  1. [All data](#df_all)\n",
    "  1. [All execution time data](#df_time)\n",
    "  1. [Mean execution time per batch](#df_mean_time_per_batch)\n",
    "  1. [Mean execution time per image](#df_mean_time_per_image)\n",
    "  1. [Best mean execution time per image](#df_best_mean_time_per_image)\n",
    "1. [See the graphs - grouped by models](#plot_models)\n",
    "  1. [All libs](#plot_models_all)\n",
    "  1. [GPU libs](#plot_models_gpu)\n",
    "  1. [CUDA-level performance libs](#plot_models_cuda)\n",
    "  1. [cuBLAS libs](#plot_models_cublas)\n",
    "  1. [cuDNN libs](#plot_models_cudnn)\n",
    "1. [See the graphs - grouped by libs](#plot_libs)\n",
    "  1. [All models](#plot_libs_all)\n",
    "  1. [All models, GPU libs](#plot_libs_gpu)\n",
    "  1. [Models with AlexNet-level accuracy](#plot_libs_alexnet)\n",
    "  1. [Models with AlexNet-level accuracy, CPU lib](#plot_libs_alexnet_cpu) \n",
    "  1. [Models with AlexNet-level accuracy, CUDA-level perfomance libs](#plot_libs_alexnet_cuda)  \n",
    "  1. [Models with AlexNet-level accuracy, libDNN libs](#plot_libs_alexnet_libdnn)\n",
    "  1. [Models with AlexNet-level accuracy, OpenCL libs](#plot_libs_alexnet_opencl)\n",
    "1. [See the graphs - per layer execution time profiling](#plot_per_layer)\n",
    "1. [See the graphs - the ideal adaptive solution](#plot_ideal)\n",
    "  1. [Using all libs for adaptation](#plot_ideal_all)\n",
    "  1. [Using CUDA-level performance libs for adaptation](#plot_ideal_cuda)\n",
    "  1. [Using cuDNN and cuBLAS for adaptation](#plot_ideal_cudnn_cublas)\n",
    "  1. [Using cuDNN and libDNN for adaptation](#plot_ideal_cudnn_libdnn)\n",
    "1. [See the memory consumption graph](#plot_memory)\n",
    "  1. [Balance memory consumption and execution time per image](#balance_memory_time)\n",
    "1. [Compare AlexNet and SqueezeNet 1.1](#alexnet_vs_squeezenet)\n",
    "  1. [Compare memory consumption](#alexnet_vs_squeezenet_memory)\n",
    "  1. [Compare execution time](#alexnet_vs_squeezenet_time)\n",
    "1. [Conclusion](#conclusion)\n",
    "  1. [What are the improvements brought on by each approach?](#improvements_of_each_approach)\n",
    "1. [Crowdsourcing benchmarking and optimisation of AI](#cknowledge_ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook compares the performance (execution time, memory consumption):\n",
    "- on **[dividiti](http://dividiti.com)**'s **velociti** Hewlett-Packard Z640 Workstation ([G1X62EA](http://h20195.www2.hp.com/v2/default.aspx?cc=ie&lc=en&oid=7528701)):\n",
    "  - [Intel(R) Xeon(R) CPU E5-2650 v3](http://ark.intel.com/products/81705/Intel-Xeon-Processor-E5-2650-v3-25M-Cache-2_30-GHz):\n",
    "    - 10 cores, 20 threads;\n",
    "    - Base clock 2300 MHz, turbo clock 3000 MHz;\n",
    "    - Max power consumption 105 Watt;\n",
    "    - Max memory bandwidth 68 GB/s;\n",
    "    - RAM memory 32 GB DDR4;\n",
    "```\n",
    "$ uname -a\n",
    "Linux velociti 4.4.0-45-generic #66-Ubuntu SMP Wed Oct 19 14:12:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n",
    "$ cat /etc/lsb-release\n",
    "DISTRIB_ID=Ubuntu\n",
    "DISTRIB_RELEASE=16.04\n",
    "DISTRIB_CODENAME=xenial\n",
    "DISTRIB_DESCRIPTION=\"Ubuntu 16.04.1 LTS\"\n",
    "```\n",
    "  - [NVIDIA GeForce GTX 1080 \"Founders Edition\"](http://www.geforce.co.uk/hardware/10series/geforce-gtx-1080/):\n",
    "    - Pascal architecture;\n",
    "    - 2560 CUDA cores;\n",
    "    - Base clock 1607 MHz, boost clock 1733 MHz;\n",
    "    - Max power consumption 180 Watt;\n",
    "    - RAM memory 8 GB GDDR5X;\n",
    "    - Max memory bandwidth 320 GB/s;\n",
    "    - GPU Driver 367.57 [10/Oct/2016];\n",
    "    - CUDA Toolkit 8.0.44 [xx/Sep/2016].\n",
    "\n",
    "- using 14 Caffe **libraries**:\n",
    "  - [`tag`] **Branch** (**revision hash, date**): **math libraries**.\n",
    "  - [`cpu`] Master ([4ba654f](https://github.com/BVLC/caffe/commit/4ba654f5c88c36ee8ba53964b7faf25c6d7010b4), 5/Oct/2016): with [OpenBLAS 0.2.19](https://github.com/xianyi/OpenBLAS/releases/tag/v0.2.19);\n",
    "  - [`cuda`] Master ([4ba654f](https://github.com/BVLC/caffe/commit/4ba654f5c88c36ee8ba53964b7faf25c6d7010b4), 5/Oct/2016): with [cuBLAS](https://developer.nvidia.com/cublas) (part of CUDA Toolkit 8.0.44);\n",
    "  - [`cudnn`] Master ([4ba654f](https://github.com/BVLC/caffe/commit/4ba654f5c88c36ee8ba53964b7faf25c6d7010b4), 5/Oct/2016): with [cuDNN 5.1](https://developer.nvidia.com/cudnn);\n",
    "  - [`nvidia-cuda`] NVIDIA v0.15 ([1024d34](https://github.com/NVIDIA/caffe/commit/1024d34d93cd34a9013d6fac4e56e45162073d38), 17/Nov/2016): with [cuBLAS](https://developer.nvidia.com/cublas) (part of CUDA Toolkit 8.0.44);\n",
    "  - [`nvidia-cudnn`] NVIDIA v0.15 ([1024d34](https://github.com/NVIDIA/caffe/commit/1024d34d93cd34a9013d6fac4e56e45162073d38), 17/Nov/2016): with [cuDNN 5.1](https://developer.nvidia.com/cudnn);\n",
    "  - [`nvidia-fp16-cuda`] NVIDIA experimental/fp16 ([fca1cf4](https://github.com/NVIDIA/caffe/commit/fca1cf475d1d0a6d355f8b9877abcc4e13951c9c), 11/Jul/2016): with [cuBLAS](https://developer.nvidia.com/cublas) (part of CUDA Toolkit 8.0.44);\n",
    "  - [`nvidia-fp16-cudnn`] NVIDIA experimental/fp16 ([fca1cf4](https://github.com/NVIDIA/caffe/commit/fca1cf475d1d0a6d355f8b9877abcc4e13951c9c), 11/Jul/2016): with [cuDNN 5.1](https://developer.nvidia.com/cudnn);\n",
    "  - [`clblas`] OpenCL ([9abafdc](https://github.com/BVLC/caffe/commit/9abafdca7b91ff5cd6f29035fdc882c269409f27), 7/Oct/2016): with [ViennaCL 1.7.1](https://github.com/viennacl/viennacl-dev/releases/tag/release-1.7.1) and [clBLAS 2.10](https://github.com/clMathLibraries/clBLAS/releases/tag/v2.10);\n",
    "  - [`clblast`] OpenCL ([9abafdc](https://github.com/BVLC/caffe/commit/9abafdca7b91ff5cd6f29035fdc882c269409f27), 7/Oct/2016): with [ViennaCL 1.7.1](https://github.com/viennacl/viennacl-dev/releases/tag/release-1.7.1) and [CLBlast 0.9.0](https://github.com/CNugteren/CLBlast/releases/tag/0.9.0);\n",
    "  - [`viennacl`] OpenCL ([9abafdc](https://github.com/BVLC/caffe/commit/9abafdca7b91ff5cd6f29035fdc882c269409f27), 7/Oct/2016): with [ViennaCL 1.7.1](https://github.com/viennacl/viennacl-dev/releases/tag/release-1.7.1) only;\n",
    "  - [`libdnn-cuda`] OpenCL ([cfaaae1](https://github.com/BVLC/caffe/commit/cfaaae1e8c95cc742d045dab2099c8404e726686), 25/Oct/2016): with [libDNN](https://github.com/BVLC/caffe/issues/4155) and [cuBLAS](https://developer.nvidia.com/cublas);\n",
    "  - [`libdnn-clblas`] OpenCL ([cfaaae1](https://github.com/BVLC/caffe/commit/cfaaae1e8c95cc742d045dab2099c8404e726686), 25/Oct/2016): with [libDNN](https://github.com/BVLC/caffe/issues/4155), [ViennaCL 1.7.1](https://github.com/viennacl/viennacl-dev/releases/tag/release-1.7.1) and [clBLAS 2.10](https://github.com/clMathLibraries/clBLAS/releases/tag/v2.10);\n",
    "  - [`libdnn-clblast`] OpenCL ([cfaaae1](https://github.com/BVLC/caffe/commit/cfaaae1e8c95cc742d045dab2099c8404e726686), 25/Oct/2016): with [libDNN](https://github.com/BVLC/caffe/issues/4155), [ViennaCL 1.7.1](https://github.com/viennacl/viennacl-dev/releases/tag/release-1.7.1) and [CLBlast 0.9.0](https://github.com/CNugteren/CLBlast/releases/tag/0.9.0);\n",
    "  - [`libdnn-viennacl`] OpenCL ([cfaaae1](https://github.com/BVLC/caffe/commit/cfaaae1e8c95cc742d045dab2099c8404e726686), 25/Oct/2016): with [libDNN](https://github.com/BVLC/caffe/issues/4155) and [ViennaCL 1.7.1](https://github.com/viennacl/viennacl-dev/releases/tag/release-1.7.1).\n",
    "  \n",
    "- using 4 CNN **models**:\n",
    "  - [GoogleNet](https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet);\n",
    "  - [AlexNet](https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet);\n",
    "  - [SqueezeNet 1.0](https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.0);\n",
    "  - [SqueezeNet 1.1](https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1);\n",
    "\n",
    "- with the **batch size** varying from 2 to 16 with step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the execution time metric to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fw = [ 'forward' ]\n",
    "fwbw = [ 'forward', 'backward' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set to fw for inference; to fwbw for training.\n",
    "direction = fw\n",
    "direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if direction==fw:\n",
    "    time_ms = 'time_fw_ms'\n",
    "else: # direction==fwbw\n",
    "    time_ms = 'time_fwbw_ms'\n",
    "time_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def images_per_second(time_in_milliseconds):\n",
    "    return 1000.0 / time_in_milliseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"code\"></a>\n",
    "## Data wrangling code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Please ignore this section if you are not interested in re-running or modifying this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Includes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scientific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some of the scientific packages are missing, please install them using:\n",
    "```\n",
    "# pip install jupyter pandas numpy matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import IPython as ip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('IPython version: %s' % ip.__version__)\n",
    "print ('Pandas version: %s' % pd.__version__)\n",
    "print ('NumPy version: %s' % np.__version__)\n",
    "print ('Matplotlib version: %s' % mp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_title = 'NVIDIA GTX 1080'\n",
    "default_ylabel = 'Execution time (ms)'\n",
    "default_colormap = cm.autumn\n",
    "default_fontsize = 16\n",
    "default_figsize = [16, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collective Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CK is not installed, please install it using:\n",
    "```\n",
    "# pip install ck\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ck.kernel as ck\n",
    "print ('CK version: %s' % ck.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repo_uoa = 'ck-caffe-nvidia-gtx1080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_experimental_results(repo_uoa, tags):\n",
    "    module_uoa = 'experiment'\n",
    "    r = ck.access({'action':'search', 'repo_uoa':repo_uoa, 'module_uoa':module_uoa, 'tags':tags})\n",
    "    if r['return']>0:\n",
    "        print (\"Error: %s\" % r['error'])\n",
    "        exit(1)\n",
    "    experiments = r['lst']\n",
    "    \n",
    "    dfs = []\n",
    "    for experiment in experiments:\n",
    "        data_uoa = experiment['data_uoa']\n",
    "        r = ck.access({'action':'list_points', 'repo_uoa':repo_uoa, 'module_uoa':module_uoa, 'data_uoa':data_uoa})\n",
    "        if r['return']>0:\n",
    "            print (\"Error: %s\" % r['error'])\n",
    "            exit(1)\n",
    "\n",
    "        # Get (lib_tag, model_tag) from a list of tags that should be available in r['dict']['tags'].\n",
    "        # Tags include 2 of the 3 irrelevant tags, a model tag and a lib tag.\n",
    "        # NB: Since it's easier to list all model tags than all lib tags, the latter list is not expicitly specified.\n",
    "        tags = r['dict']['tags']\n",
    "        irrelevant_tags = [ 'explore-batch-size-libs-models','time_gpu','time_cpu','time_gpu_fp16' ]\n",
    "        model_tags = [ 'bvlc-alexnet','bvlc-googlenet','deepscale-squeezenet-1.0','deepscale-squeezenet-1.1' ]\n",
    "        lib_model_tags = [ tag for tag in tags if tag not in irrelevant_tags ]\n",
    "        model_tags = [ tag for tag in lib_model_tags if tag in model_tags ]\n",
    "        lib_tags = [ tag for tag in lib_model_tags if tag not in model_tags ]\n",
    "        if len(lib_tags)==1 and len(model_tags)==1:\n",
    "            (lib, model) = (lib_tags[0], model_tags[0])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        for point in r['points']:\n",
    "            with open(os.path.join(r['path'], 'ckp-%s.0001.json' % point)) as point_file:\n",
    "                point_data_raw = json.load(point_file)\n",
    "                # Obtain column data.\n",
    "                characteristics = [\n",
    "                    {\n",
    "                        'time (ms)'   : characteristics['run'].get(time_ms,+1e9), # \"positive infinity\"\n",
    "                        'memory (MB)' : characteristics['run'].get('memory_mbytes',-1),\n",
    "                        'success?'    : characteristics['run'].get('run_success','n/a'),\n",
    "                        'per layer info' : characteristics['run'].get('per_layer_info',[])\n",
    "                    }\n",
    "                    for characteristics in point_data_raw['characteristics_list'] \n",
    "                ]\n",
    "                # Deal with missing column data (resulting from failed runs).\n",
    "                if len(characteristics)==1:\n",
    "                    repetitions = point_data_raw['features'].get('statistical_repetitions',1)\n",
    "                    characteristics = characteristics * repetitions\n",
    "                # Construct a DataFrame.\n",
    "                df = pd.DataFrame(characteristics)\n",
    "                # Set columns and index names.\n",
    "                df.columns.name = 'run characteristic'\n",
    "                df.index.name = 'repetition'\n",
    "                # Set indices.\n",
    "                df['lib'] = lib\n",
    "                df['model'] = model\n",
    "                df['batch size'] = point_data_raw['choices']['env']['CK_CAFFE_BATCH_SIZE']\n",
    "                df = df.set_index(['lib', 'model', 'batch size'], append=True)\n",
    "                df = df.reorder_levels(('model', 'lib', 'batch size', 'repetition'))\n",
    "                # Append to the list of similarly constructed DataFrames.\n",
    "                dfs.append(df)\n",
    "    # Concatenate all constructed DataFrames (i.e. stack on top of each other).\n",
    "    result = pd.concat(dfs)\n",
    "    return result.sortlevel(result.index.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot execution time per image or memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot(mean, std, title=default_title, ylabel=default_ylabel, rot=0, ymax=0):\n",
    "    ymax = mean.max().max() if ymax==0 else ymax\n",
    "    ax = mean.plot(kind='bar', yerr=std, grid=True, legend=True, rot=rot, ylim=[0,ymax*1.05],\n",
    "                   fontsize=default_fontsize, figsize=default_figsize, colormap=default_colormap)\n",
    "    ax.set_title(title, fontsize=default_fontsize)\n",
    "    ax.set_xlabel(mean.index.name, fontsize=default_fontsize)\n",
    "    ax.set_ylabel(ylabel, fontsize=default_fontsize)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot maximum number of images per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretty_print_libs = {\n",
    "    'cpu': 'OpenBLAS (CPU)',\n",
    "    'libdnn-cuda':'libDNN-fp32',\n",
    "    'nvidia-cuda':'cuBLAS-fp32',\n",
    "    'nvidia-fp16-cuda':'cuBLAS-fp16',\n",
    "    'nvidia-cudnn':'cuDNN-fp32',\n",
    "    'nvidia-fp16-cudnn':'cuDNN-fp16'\n",
    "}\n",
    "\n",
    "pretty_print_models = {\n",
    "    'bvlc-alexnet':'AlexNet',\n",
    "    'bvlc-googlenet':'GoogleNet',\n",
    "    'deepscale-squeezenet-1.0':'SqueezeNet 1.0',\n",
    "    'deepscale-squeezenet-1.1':'SqueezeNet 1.1'\n",
    "}\n",
    "\n",
    "speedup_sort_models = [\n",
    "    'OpenBLAS (CPU)',\n",
    "    'libDNN-fp32',\n",
    "    'cuBLAS-fp32',\n",
    "    'cuBLAS-fp16',\n",
    "    'cuDNN-fp32',\n",
    "    'cuDNN-fp16'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ['cuda', 'cudnn'] are roughly equivalent to ['nvidia-cuda', 'nvidia-cudnn'], so can be dropped.\n",
    "def plot_max_num_images_per_second(df_mean_time_per_image, libs_to_drop=['cuda', 'cudnn'], rot=0, fontsize=None):\n",
    "    min_time_per_image = df_mean_time_per_image.min(axis=1).unstack('lib')\n",
    "    max_num_images_per_second = images_per_second(min_time_per_image) \\\n",
    "        .drop(libs_to_drop, axis=1) \\\n",
    "        .rename(columns=pretty_print_libs, index=pretty_print_models) \\\n",
    "        .reindex(columns=speedup_sort_models)\n",
    "    ax = max_num_images_per_second \\\n",
    "        .plot(kind='bar', rot=rot, width=0.95, grid=True, legend=True,\n",
    "              fontsize=default_fontsize, figsize=default_figsize, colormap=default_colormap)\n",
    "    ax.set_title(default_title, fontsize=default_fontsize)\n",
    "    ax.set_xlabel(max_num_images_per_second.index.name, fontsize=default_fontsize)\n",
    "    ax.set_ylabel('Images/s (with the best even batch size between 2 and 16)', fontsize=default_fontsize)\n",
    "    ax.legend(loc='upper center');\n",
    "    for patch in ax.patches:\n",
    "         ax.annotate(str(int(patch.get_height()+0.5)), (patch.get_x()*1.00, patch.get_height()*1.01), fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the speedup over a given baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ['cuda', 'cudnn'] are roughly equivalent to ['nvidia-cuda', 'nvidia-cudnn'], so can be dropped.\n",
    "def plot_speedup_over_baseline(df_mean_time_per_image, baseline='cpu', libs_to_drop=['cuda', 'cudnn'], rot=0, fontsize=None):\n",
    "    speedup_over_baseline = df_mean_time_per_image.min(axis=1).unstack('model').ix[baseline] / \\\n",
    "                            df_mean_time_per_image.min(axis=1).unstack('model')\n",
    "    speedup_over_baseline = speedup_over_baseline.T \\\n",
    "        .drop(libs_to_drop, axis=1) \\\n",
    "        .rename(index=pretty_print_models, columns=pretty_print_libs) \\\n",
    "        .reindex(columns=speedup_sort_models)\n",
    "    ax = speedup_over_baseline \\\n",
    "        .plot(kind='bar', rot=rot, width=0.95, grid=True, legend=True,\n",
    "              fontsize=default_fontsize, figsize=default_figsize, colormap=default_colormap)\n",
    "    ax.set_title(default_title, fontsize=default_fontsize)\n",
    "    ax.set_xlabel(speedup_over_baseline.index.name, fontsize=default_fontsize)\n",
    "    ax.set_ylabel('Speedup over the given baseline (%s)' % pretty_print_libs[baseline], fontsize=default_fontsize)\n",
    "    for patch in ax.patches:\n",
    "        ax.annotate('{0:.2f}'.format(patch.get_height())[0:4], (patch.get_x()*1.00, patch.get_height()*1.01),\n",
    "                        fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot execution time per image per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This transformation is time consuming, hence only call it once for multiple plots.\n",
    "def get_per_layer_info(df_all):\n",
    "    df_per_layer_info = df_all['per layer info']\n",
    "    row_dfs = []\n",
    "    for (row_info, row_id) in zip(df_per_layer_info, range(len(df_per_layer_info))):\n",
    "        # Skip constructing a DataFrame when no layer info is available.\n",
    "        if not row_info: continue\n",
    "        # Augment each layer info with the row index: (model, lib, batch size, repetition).\n",
    "        for layer_info in row_info:\n",
    "            layer_info.update({ k : v for k, v in zip(df_per_layer_info.index.names, df_per_layer_info.index[row_id]) })\n",
    "        # Construct a DataFrame and move the row index to where it belongs.\n",
    "        row_df = pd.DataFrame(data=row_info).set_index(df_per_layer_info.index.names)\n",
    "        row_dfs.append(row_df)\n",
    "    return pd.concat(row_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_time_per_image_per_layer(df_per_layer_info, model, libs, batch_sizes,\n",
    "                                  direction=['forward'], lower=0.0, upper=1.0, ymax=0, rot=90):\n",
    "    df_time_per_batch = df_per_layer_info.loc[model, libs, batch_sizes] \\\n",
    "        .set_index(['direction', 'label'], append=True) \\\n",
    "        .reorder_levels(['direction', 'label', 'model', 'lib', 'batch size', 'repetition' ]) \\\n",
    "        .ix[direction] \\\n",
    "        .reorder_levels(['label', 'model', 'lib', 'batch size', 'repetition', 'direction' ]) \\\n",
    "        .groupby(level=['label', 'model', 'lib', 'batch size', 'repetition']).sum() \\\n",
    "        ['time_ms']\n",
    "    df_time_per_image = df_time_per_batch.unstack('batch size') / batch_sizes\n",
    "    df = df_time_per_image.unstack(['lib', 'model'])\n",
    "    df = df.reorder_levels(['model', 'lib', 'batch size'], axis=1)\n",
    "    mean = df.groupby(level='label').mean()\n",
    "    std = df.groupby(level='label').std()\n",
    "    select = (lower*mean.sum() <= mean).any(axis=1) & (mean <= upper*mean.sum()).any(axis=1)\n",
    "    ymax = mean[select].max().max() if ymax==0 else ymax\n",
    "    ax = plot(mean=mean[select], std=std[select], ylabel='Execution per image time per layer (ms)', ymax=ymax, rot=rot)\n",
    "    ax.set_xlabel('Layer', fontsize=default_fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ideal adaptive solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The ideal adaptive solution for each layer selects the best performing library from the 'libs_for_adaptation' list.\n",
    "# FIXME: add batch_sizes as explicit parameter.\n",
    "def get_ideal_adaptive_solution(df_per_layer_info, libs_for_adaptation, direction):\n",
    "    df_for_adaptation = df_per_layer_info \\\n",
    "        .set_index(['direction', 'label'], append=True) \\\n",
    "        .reorder_levels(['direction', 'lib', 'model', 'label', 'batch size', 'repetition']) \\\n",
    "        .ix[direction] \\\n",
    "        .reorder_levels(['lib', 'model', 'label', 'batch size', 'repetition', 'direction']) \\\n",
    "        .ix[libs_for_adaptation] \\\n",
    "        .reorder_levels(['model', 'label', 'lib', 'batch size', 'repetition', 'direction']) \\\n",
    "        ['time_ms']\n",
    "    # With every step, reduce the rightmost dimension until the min time per model is reached.\n",
    "    df_cum_time_per_repetition = df_for_adaptation.groupby(level=df_for_adaptation.index.names[:-1]).sum()\n",
    "    df_min_time_per_repetition = df_cum_time_per_repetition.groupby(level=df_cum_time_per_repetition.index.names[:-1]).min()\n",
    "    df_min_time_per_batch = df_min_time_per_repetition.unstack('batch size') / batch_sizes\n",
    "    df_min_time_per_image = df_min_time_per_batch.min(axis=1)\n",
    "    df_min_time_per_layer = df_min_time_per_image.groupby(level=df_min_time_per_image.index.names[:-1]).min()\n",
    "    #df_min_time_per_model = df_min_time_per_layer.groupby(level=df_min_time_per_layer.index.names[:-1]).sum()\n",
    "    # Transform to get the models in the index and the libs in the columns.\n",
    "    df_min_time_per_layer_idx = df_min_time_per_image.groupby(level=df_min_time_per_image.index.names[:-1]).idxmin()\n",
    "    df_ideal = df_min_time_per_image[df_min_time_per_layer_idx] \\\n",
    "        .reorder_levels(['model', 'lib', 'label']) \\\n",
    "        .groupby(level=['model', 'lib']).sum() \\\n",
    "        .unstack('lib')\n",
    "    # Sort in the order of increasing time per model.\n",
    "    df_ideal_sorted = df_ideal.ix[df_ideal.sum(axis=1).sort_values(ascending=True).index]\n",
    "    return df_ideal_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_ideal_adaptive_solution(df_ideal, df_real, tag=\"\"):\n",
    "    figsize=[15, 3]\n",
    "    if not tag==\"\": figsize=[10, 2] # good for dumping png (e.g. 3 graphs fit well onto a slide).\n",
    "    for model in df_ideal.index:\n",
    "        df_data = {}; df_data['adaptive'] = df_ideal.ix[model]\n",
    "        for lib in df_ideal.columns:\n",
    "            df_data[lib] = pd.Series(index=df_ideal.columns)\n",
    "            df_data[lib][lib] = df_real.ix[model, lib]\n",
    "        df = pd.DataFrame(df_data).T \\\n",
    "            .rename(index={'cpu': 'OpenBLAS only', 'cuda':'cuBLAS only', 'cudnn':'cuDNN only', 'libdnn-cuda': 'libDNN only'},\n",
    "                    columns={'cpu': 'OpenBLAS', 'cuda':'cuBLAS', 'cudnn':'cuDNN', 'libdnn-cuda': 'libDNN'})\n",
    "        ax = df.ix[df.sum(axis=1).sort_values(ascending=True).index] \\\n",
    "            .plot(kind='barh', stacked=True, width=0.9, grid=True, legend=True,\n",
    "                  fontsize=default_fontsize, figsize=figsize, colormap=cm.summer_r)\n",
    "            #.legend(loc='lower right')\n",
    "        ax.set_title('%s - execution time per image (ms)' % model, fontsize=default_fontsize)\n",
    "        if not tag==\"\": ax.get_figure().savefig('%s.%s.png' % (tag, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot execution time per image and memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_time_per_image_and_memory_consumption(df_all, model, lib):\n",
    "    df = df_all[['time (ms)', 'memory (MB)']] \\\n",
    "        .groupby(level=df_all.index.names[:-1]).mean() \\\n",
    "        .loc[model, lib]\n",
    "    df['time per image (ms)'] = df['time (ms)'].divide(df.index, axis=0)\n",
    "    df['memory per image (MB)'] = df['memory (MB)'].divide(df.index, axis=0)\n",
    "    df = df.drop('time (ms)', axis=1).sortlevel(axis=1)\n",
    "    ax = df.plot(secondary_y=['memory (MB)', 'memory per image (MB)'], mark_right=False, grid=True,\n",
    "                 figsize=[12, 8], fontsize=default_fontsize, colormap=cm.winter)\n",
    "    ax.set_title('%s w/ %s' % (model, lib), fontsize=default_fontsize)\n",
    "    ax.set_xlabel(df.index.name, fontsize=default_fontsize)\n",
    "    ax.set_ylabel('execution time (ms)', fontsize=default_fontsize); ax.legend(loc='center left'); ax.set_ylim(0)\n",
    "    ax.right_ax.set_ylabel('memory consumption (MB)', fontsize=default_fontsize); ax.right_ax.legend(loc='center right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## Get the experimental data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Please ignore this section if you are not interested in re-running or modifying this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Caffe experimental data was collected on the experimental platform (after installing all Caffe libraries and models of interest) as follows:\n",
    "```\n",
    "$ cd `ck find ck-caffe:script:explore-batch-size-libs-models`\n",
    "$ python explore-batch-size-libs-models-benchmark.py\n",
    "```\n",
    "It can be downloaded from GitHub via CK as follows:\n",
    "```\n",
    "$ ck pull repo:ck-caffe-nvidia-gtx1080 --url=https://github.com/dividiti/ck-caffe-nvidia-gtx1080\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tables\"></a>\n",
    "## Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"df_all\"></a>\n",
    "### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all = get_experimental_results(repo_uoa=repo_uoa, tags='explore-batch-size-libs-models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = len(df_all.columns)\n",
    "pd.options.display.max_rows = len(df_all.index)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"df_time\"></a>\n",
    "### All execution time data indexed by repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_time = df_all['time (ms)'].unstack(df_all.index.names[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = len(df_time.columns)\n",
    "pd.options.display.max_rows = len(df_time.index)\n",
    "df_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"df_mean_time_per_batch\"></a>\n",
    "### Mean execution time per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_mean_time_per_batch = df_time.describe().ix['mean'].unstack(level='batch size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = len(df_mean_time_per_batch.columns)\n",
    "pd.options.display.max_rows = len(df_mean_time_per_batch.index)\n",
    "df_mean_time_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_sizes = df_mean_time_per_batch.columns.tolist()\n",
    "# batch_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"df_mean_time_per_image\"></a>\n",
    "### Mean execution time per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_mean_time_per_image = df_mean_time_per_batch / batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = len(df_mean_time_per_image.columns)\n",
    "pd.options.display.max_rows = len(df_mean_time_per_image.index)\n",
    "df_mean_time_per_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"df_best_mean_time_per_image\"></a>\n",
    "### Best mean execution time per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_mean_time_per_image.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_max_num_images_per_second(df_mean_time_per_image, libs_to_drop=[], fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What is the batch size that gives the minimum time per image (or the maximum number of images per second)?\n",
    "df_mean_time_per_image.idxmin(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Focus on e.g. nvidia-fp16-cuda, for which the batch size of 16 is not always the best.\n",
    "df_mean_time_per_image.idxmin(axis=1).reorder_levels(['lib', 'model']).loc['nvidia-fp16-cuda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Is the same answer as via .min(axis=1).values?\n",
    "# df_mean_time_per_image.lookup(df_mean_time_per_image.index, df_mean_time_per_image.idxmin(axis=1)) \\\n",
    "#     == df_mean_time_per_image.min(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_time_per_image = df_time / (batch_sizes*(len(df_time.columns)/len(batch_sizes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_min_time_per_image_index = pd.DataFrame(df_mean_time_per_image.idxmin(axis=1)).set_index(0, append=True).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_model_lib = df_time_per_image[df_min_time_per_image_index] \\\n",
    "    .stack(['model', 'lib']).reorder_levels(['model','lib','repetition']).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_model_lib_mean = df_model_lib.groupby(level=['model', 'lib']).mean()\n",
    "df_model_lib_std  = df_model_lib.groupby(level=['model', 'lib']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_positive_infinity = df_model_lib_mean > 1e5\n",
    "df_model_lib_mean[zero_positive_infinity] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exclude_positive_infinity = df_model_lib_mean < 1e6\n",
    "# df_model_lib_mean = df_model_lib_mean[exclude_positive_infinity]\n",
    "# df_model_lib_std = df_model_lib_std[exclude_positive_infinity]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_models\"></a>\n",
    "## Plot by Caffe models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_models_all\"></a>\n",
    "### All libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.unstack('lib')\n",
    "std  = df_model_lib_std.unstack('lib')\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_models_gpu\"></a>\n",
    "### Only GPU libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.unstack('lib').drop('cpu', axis=1)\n",
    "std  = df_model_lib_std.unstack('lib').drop('cpu', axis=1)\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_models_cuda\"></a>\n",
    "### Only GPU libs with CUDA-level fp32 performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda_level_performance = ['nvidia-cuda', 'nvidia-cudnn', 'libdnn-cuda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.reorder_levels(['lib', 'model'])[cuda_level_performance].unstack('lib')\n",
    "std = df_model_lib_std.reorder_levels(['lib', 'model'])[cuda_level_performance].unstack('lib')\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_models_cublas\"></a>\n",
    "### Only GPU libs using cuBLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cublas_libs = ['cuda', 'nvidia-cuda', 'nvidia-fp16-cuda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.reorder_levels(['lib', 'model'])[cublas_libs].unstack('lib')\n",
    "std = df_model_lib_std.reorder_levels(['lib', 'model'])[cublas_libs].unstack('lib')\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With cuBLAS, BVLC's master is up to 11% slower than NVIDIA's mainline.\n",
    "cuda_vs_nvidia_cuda = mean['cuda'] / mean['nvidia-cuda']\n",
    "cuda_vs_nvidia_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With cuBLAS, NVIDIA's fp16 branch is up to 77% slower than NVIDIA's fp32 mainline. Some of this difference\n",
    "# may be explained by the fp16 branch not being maintained, hence not including the latest improvements.\n",
    "# NB: But see below a more drastic performance difference and info on GTX 1080 support of fp16.\n",
    "nvidia_fp16_cuda_vs_nvidia_fp32_cuda = mean['nvidia-fp16-cuda'] / mean['nvidia-cuda']\n",
    "nvidia_fp16_cuda_vs_nvidia_fp32_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_models_cudnn\"></a>\n",
    "### Only GPU libs using cuDNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cudnn_libs = ['cudnn', 'nvidia-cudnn', 'nvidia-fp16-cudnn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.reorder_levels(['lib', 'model'])[cudnn_libs].unstack('lib')\n",
    "std = df_model_lib_std.reorder_levels(['lib', 'model'])[cudnn_libs].unstack('lib')\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With cuDNN, BVLC's master is between 8% faster and 5% slower than NVIDIA's mainline (i.e. they are roughly equivalent).\n",
    "cuda_vs_nvidia_cuda = mean['cudnn'] / mean['nvidia-cudnn']\n",
    "cuda_vs_nvidia_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With cuDNN, NVIDIA's fp16 branch is up to 27 times slower than NVIDIA's fp32 mainline.\n",
    "nvidia_fp16_cudnn_vs_nvidia_fp32_cudnn = mean['nvidia-fp16-cudnn'] / mean['nvidia-cudnn']\n",
    "nvidia_fp16_cudnn_vs_nvidia_fp32_cudnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.anandtech.com/show/10325/the-nvidia-geforce-gtx-1080-and-1070-founders-edition-review/5\n",
    "\n",
    "> Low precision operations are in turn seen by NVIDIA as one of the keys into further growing their increasingly important datacenter market, as deep learning and certain other tasks are themselves rapidly growing fields. Pascal isn’t just faster than Maxwell overall, but when it comes to FP16 operations on the FP16x2 core, Pascal is a **lot** faster, with theoretical throughput over similar Maxwell GPUs increasing by over three-fold thanks to the combination of overall speed improvements and double speed FP16 execution.\n",
    "\n",
    "> GeForce GTX 1080, on the other hand, is **not** faster at FP16. In fact it’s downright slow. For their consumer cards, NVIDIA has severely limited FP16 CUDA performance. GTX 1080’s FP16 instruction rate is 1/128th its FP32 instruction rate, or after you factor in vec2 packing, the resulting theoretical performance (in FLOPs) is 1/64th the FP32 rate, or about 138 GFLOPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs\"></a>\n",
    "## Plot by Caffe libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs_all\"></a>\n",
    "### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.unstack('model')\n",
    "std  = df_model_lib_std.unstack('model')\n",
    "plot(mean, std, rot=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs_gpu\"></a>\n",
    "### All models, only GPU libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.unstack('model').drop('cpu', axis=0)\n",
    "std  = df_model_lib_std.unstack('model').drop('cpu', axis=0)\n",
    "plot(mean, std, rot=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs_alexnet\"></a>\n",
    "### Only models with AlexNet-level accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alexnet_level_accuracy = ['bvlc-alexnet','deepscale-squeezenet-1.0','deepscale-squeezenet-1.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On this platform with all the libraries, SqueezeNet 1.0 is always slower than AlexNet\n",
    "# despite a 50x reduction in weights (5 MB vs. 250 MB).\n",
    "mean = df_model_lib_mean[alexnet_level_accuracy].unstack('model')\n",
    "std  = df_model_lib_std[alexnet_level_accuracy].unstack('model')\n",
    "plot(mean, std, rot=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs_alexnet_cpu\"></a>\n",
    "### Only models with AlexNet-level accuracy, only CPU lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SqueezeNet 1.1 is 41% faster than AlexNet with OpenBLAS (on the CPU).\n",
    "mean = df_model_lib_mean[alexnet_level_accuracy].unstack('model').ix[['cpu']]\n",
    "std  = df_model_lib_std[alexnet_level_accuracy].unstack('model').ix[['cpu']]\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean['deepscale-squeezenet-1.1'] / mean['bvlc-alexnet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs_alexnet_cuda\"></a>\n",
    "### Only models with AlexNet-level accuracy, only libs with CUDA-level performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SqueezeNet 1.0 is slower than AlexNet. SqueezeNet 1.1 is 18% faster than AlexNet with libDNN-CUDA.\n",
    "mean = df_model_lib_mean[alexnet_level_accuracy].unstack('model').ix[cuda_level_performance]\n",
    "std  = df_model_lib_std[alexnet_level_accuracy].unstack('model').ix[cuda_level_performance]\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean['deepscale-squeezenet-1.0'] / mean['bvlc-alexnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean['deepscale-squeezenet-1.1'] / mean['bvlc-alexnet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs_alexnet_libdnn\"></a>\n",
    "### Only models with AlexNet-level accuracy, only libDNN libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "libdnn_libs = [ 'libdnn-cuda', 'libdnn-clblas', 'libdnn-clblast', 'libdnn-viennacl' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With the libDNN libs, SqueezeNet 1.1 is roughly equivalent to AlexNet.\n",
    "mean = df_model_lib_mean[alexnet_level_accuracy].unstack('model').ix[libdnn_libs]\n",
    "std  = df_model_lib_std[alexnet_level_accuracy].unstack('model').ix[libdnn_libs]\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean['deepscale-squeezenet-1.1'] / mean['bvlc-alexnet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs_alexnet_opencl\"></a>\n",
    "### Only models with AlexNet-level accuracy, only OpenCL libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opencl_libs = [ 'clblas', 'clblast', 'viennacl' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SqueezeNet 1.0 is slower than AlexNet with all the OpenCL BLAS libs. \n",
    "# SqueezeNet 1.1 is 28% faster than AlexNet with ViennaCL and 6.5% faster with clBLAS,\n",
    "# but over 2 times slower with CLBlast.\n",
    "mean = df_model_lib_mean[alexnet_level_accuracy].unstack('model').ix[opencl_libs]\n",
    "std  = df_model_lib_std[alexnet_level_accuracy].unstack('model').ix[opencl_libs]\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean['deepscale-squeezenet-1.1'] / mean['bvlc-alexnet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_per_layer\"></a>\n",
    "## Plot execution time per image per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_per_layer_info = get_per_layer_info(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.options.display.max_columns = len(df_per_layer_info.columns)\n",
    "# pd.options.display.max_rows = len(df_per_layer_info.index)\n",
    "# df_per_layer_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot for a list of batch sizes.\n",
    "# NB: This suggests that the fully connected layers benefit the most from increasing the batch size.\n",
    "plot_time_per_image_per_layer(df_per_layer_info, model='bvlc-alexnet', libs='nvidia-cuda',\n",
    "                              batch_sizes=[2, 8, 16], direction=direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot for a list of batch sizes. Only plot layers that consume at least 10% of the total execution time.\n",
    "plot_time_per_image_per_layer(df_per_layer_info, model='bvlc-alexnet', libs='nvidia-cudnn',\n",
    "                              batch_sizes=[8, 16], direction=direction, lower=0.10, rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot for a list of libs.\n",
    "# NB: cuDNN and cuBLAS perform about the same on the fully connected layers (which suggests that\n",
    "# cuDNN falls back to cuBLAS for these).\n",
    "# Unsurprisingly, cuDNN performs better than cuBLAS on the convolution layers.\n",
    "# Surprisingly, cuBLAS performs a bit better than cuDNN on the relu layers.\n",
    "plot_time_per_image_per_layer(df_per_layer_info, model='bvlc-alexnet', libs=['nvidia-cuda','nvidia-cudnn'],\n",
    "                              batch_sizes=16, direction=direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot for a list of libs.\n",
    "# NB: This suggests that libDNN is faster than cuDNN on the expand1x1 layers, but slower on the squeeze1x1, \n",
    "# expand3x3, conv/pool10 layers.\n",
    "plot_time_per_image_per_layer(df_per_layer_info, model='deepscale-squeezenet-1.1', libs=['nvidia-cudnn', 'libdnn-cuda'],\n",
    "                              batch_sizes=16, direction=direction, ymax=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot for a list of libs. Only plot layers that consume between 5% and 10% of the total execution time.\n",
    "# NB: libDNN is slower than cuDNN on the expand3x3 layers and conv10 layers, but a bit faster on the conv1 layer.\n",
    "plot_time_per_image_per_layer(df_per_layer_info, model='deepscale-squeezenet-1.1', libs=['nvidia-cudnn', 'libdnn-cuda'],\n",
    "                              batch_sizes=16, direction=direction, lower=0.05, upper=0.10, rot=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot for a list of libs and a list of batch sizes. (This works but might not be terribly legible).\n",
    "plot_time_per_image_per_layer(df_per_layer_info, model='bvlc-alexnet', libs=['nvidia-cudnn', 'nvidia-cuda'],\n",
    "                              batch_sizes=[4,6], direction=direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_ideal\"></a>\n",
    "## Plot the ideal adaptive solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, using cuDNN typically results in the minimum execution time. For some layers, however, other libraries may outperform cuDNN (e.g. libDNN from the OpenCL branch of Caffe). As we show below, using the best performing library per layer results in up to 20% execution time reduction over using cuDNN alone. For other models and on other platforms such adaptation can potentially results even in higher savings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Currently, the savings are hypothetical. However, Caffe allows for manual adaptation, i.e. the user can specify the engine to use for each layer in the model file (`*.prototxt`). We are working on generating the optimized model file automatically from the obtained ideal adaptive solution. Please [contact us](info@dividiti.com) if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_ideal_all\"></a>\n",
    "### Using all reasonable libs for adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only include libs built from the master and OpenCL branches because per layer adaptation implies building from the same source. The OpenCL branch is kept in sync with the master, while the NVIDIA branches are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_libs = df_per_layer_info.index.get_level_values('lib').drop_duplicates() \\\n",
    "    .drop(['nvidia-cuda', 'nvidia-cudnn', 'nvidia-fp16-cuda', 'nvidia-fp16-cudnn'])\n",
    "all_libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row specifies an ideal adaptive solution for a model. Each column specifies the execution time (in ms per image) that the ideal adaptive solution would cumulatively spend using a particular library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ideal_all = get_ideal_adaptive_solution(df_per_layer_info, all_libs, direction)\n",
    "df_ideal_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ideal_adaptive_solution(df_ideal_all, df_model_lib_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Up to 20% execution time reduction compared to the best non-adaptive solution (i.e. cuDNN).\n",
    "df_best_lib = df_model_lib_mean.reorder_levels(['lib', 'model'])[cuda_level_performance].unstack('lib')\n",
    "df_ideal_all.sum(axis=1) / df_best_lib.min(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_ideal_cuda\"></a>\n",
    "### Using CUDA-level performance libs for adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ideal_cuda = get_ideal_adaptive_solution(df_per_layer_info, ['cuda', 'cudnn', 'libdnn-cuda'], direction)\n",
    "df_ideal_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ideal_adaptive_solution(df_ideal_cuda, df_model_lib_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hypothetical execution time reduction compared to the best non-adaptive solution (i.e. cuDNN).\n",
    "df_best_lib = df_model_lib_mean.reorder_levels(['lib', 'model'])[cuda_level_performance].unstack('lib')\n",
    "df_ideal_cuda.sum(axis=1) / df_best_lib.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Up to 0.1% worse performance when using the CUDA-level performance libs only.\n",
    "df_ideal_cuda.sum(axis=1) / df_ideal_all.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_ideal_cudnn_cublas\"></a>\n",
    "### Using cuDNN and cuBLAS for adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ideal_cudnn_cublas = get_ideal_adaptive_solution(df_per_layer_info, ['cudnn', 'cuda'], direction)\n",
    "df_ideal_cudnn_cublas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ideal_adaptive_solution(df_ideal_cudnn_cublas, df_model_lib_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hypothetical execution time reduction compared to the best non-adaptive solution (i.e. cuDNN).\n",
    "df_best_lib = df_model_lib_mean.reorder_levels(['lib', 'model'])[cuda_level_performance].unstack('lib')\n",
    "df_ideal_cudnn_cublas.sum(axis=1) / df_best_lib.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Up to 14% worse performance when using cuDNN+cuBLAS only.\n",
    "df_ideal_cudnn_cublas.sum(axis=1) / df_ideal_all.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_ideal_cudnn_libdnn\"></a>\n",
    "### Using cuDNN and libDNN for adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ideal_cudnn_libdnn = get_ideal_adaptive_solution(df_per_layer_info, ['cudnn', 'libdnn-cuda'], direction)\n",
    "df_ideal_cudnn_libdnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ideal_adaptive_solution(df_ideal_cudnn_libdnn, df_model_lib_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hypothetical execution time reduction compared to the best non-adaptive solution (i.e. cuDNN).\n",
    "df_best_lib = df_model_lib_mean.reorder_levels(['lib', 'model'])[cuda_level_performance].unstack('lib')\n",
    "df_ideal_cudnn_libdnn.sum(axis=1) / df_best_lib.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Less than 1% worse performance when using cuDNN+libDNN only.\n",
    "df_ideal_cudnn_libdnn.sum(axis=1) / df_ideal_all.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_memory\"></a>\n",
    "## Plot memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_memory = df_all['memory (MB)']\n",
    "# Batch size of 4; repetition 0 (should always be available).\n",
    "df_memory = df_memory.unstack(['model','lib']).loc[4].loc[0].unstack('lib')\n",
    "plot(mean=df_memory, std=pd.DataFrame(), ylabel='Memory size (MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"balance_memory_time\"></a>\n",
    "### Balance memory consumption and execution time per image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above, however, does not tell the full story. The memory consumption, as reported by Caffe, increases linearly with the batch size. In other words, the memory consumption per image is constant. (Note that extra memory may be required e.g. for GPU buffers in host memory.)\n",
    "\n",
    "The execution time per image, however, decreases asymptotically. Since minimizing the execution time almost always should be balanced with minimizing the memory consumption, we should select the batch size that results in \"good enough\" performance.\n",
    "\n",
    "We give several examples below. Note that the execution time per batch is omitted to make the execution time per image more pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Is the batch size of 8 \"good enough\"?\n",
    "plot_time_per_image_and_memory_consumption(df_all, 'bvlc-alexnet', 'nvidia-cudnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Is the batch size of 2 \"good enough\"?\n",
    "plot_time_per_image_and_memory_consumption(df_all, 'deepscale-squeezenet-1.1', 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"alexnet_vs_squeezenet\"></a>\n",
    "## Compare AlexNet and SqueezeNet 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"alexnet_vs_squeezenet_memory\"></a>\n",
    "### Memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SqueezeNet consumes about 4 times more memory than AlexNet.\n",
    "df_memory.ix[['bvlc-alexnet', 'deepscale-squeezenet-1.1']].iloc[1] / \\\n",
    "df_memory.ix[['bvlc-alexnet', 'deepscale-squeezenet-1.1']].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"alexnet_vs_squeezenet_time\"></a>\n",
    "### Execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean[['bvlc-alexnet', 'deepscale-squeezenet-1.1']].unstack('lib')\n",
    "std  = df_model_lib_std[['bvlc-alexnet', 'deepscale-squeezenet-1.1']].unstack('lib')\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_model_lib_mean[['bvlc-alexnet', 'deepscale-squeezenet-1.1']].unstack('lib').iloc[1] / \\\n",
    "df_model_lib_mean[['bvlc-alexnet', 'deepscale-squeezenet-1.1']].unstack('lib').iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"improvements_of_each_approach\"></a>\n",
    "### What are the improvements brought on by each approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cuDNN-fp32 is up to 129x faster than the CPU.\n",
    "plot_speedup_over_baseline(df_mean_time_per_image, baseline='cpu', libs_to_drop=[], fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cuDNN-fp32 is up to 3.1x faster than cuBLAS-fp32.\n",
    "plot_speedup_over_baseline(df_mean_time_per_image, baseline='nvidia-cuda', libs_to_drop=[], fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AlexNet and SqueezeNet 1.1 have very similar performance with cuBLAS-fp32 and cuDNN-fp32, as well as very similar accuracy!\n",
    "# At the same time, SqueezeNet requires about 4 times more activation memory and 50 times less memory for \n",
    "# the weights than AlexNet.\n",
    "plot_speedup_over_baseline(df_mean_time_per_image.ix[['bvlc-alexnet', 'deepscale-squeezenet-1.1']],\n",
    "                           baseline='nvidia-cuda', libs_to_drop=[], fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cknowledge_ai\"></a>\n",
    "## [cknowledge.org/ai](https://cknowledge.org/ai): Crowdsourcing benchmarking and optimisation of AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A suite of open-source tools for [collecting knowledge on optimising AI](http://bit.ly/hipeac49-ckdl):\n",
    "\n",
    "\n",
    "* [Android app](https://play.google.com/store/apps/details?id=openscience.crowdsource.video.experiments&hl=en_GB)\n",
    "* [Desktop app](https://github.com/dividiti/ck-crowdsource-dnn-optimization)\n",
    "* [CK-Caffe](https://github.com/dividiti/ck-caffe)\n",
    "* [CK-Caffe2](https://github.com/ctuning/ck-caffe2)\n",
    "* [CK-TensorRT](https://github.com/dividiti/ck-tensorrt)\n",
    "* [CK-TensorFlow](https://github.com/ctuning/ck-tensorflow)\n",
    "* etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
